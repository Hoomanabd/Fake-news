{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapping full articles\n",
    "##### Proceed in several rounds (indicated by R) for optimal use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### R1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('scraped_data1.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Sources', 'no1']] = df.Sources.str.split(\"-\",expand=True,)\n",
    "df[['no2', 'Sources']] = df.Sources.str.split(\"By \",expand=True,)\n",
    "df[['no3', 'Dates']] = df.Dates.str.split(\"-\",expand=True,)\n",
    "df= df[['Headlines','Sources','Dates','URLs']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store the scraped full articles\n",
    "article_dates = []\n",
    "full_articles = []\n",
    "\n",
    "\n",
    "# Loop over each URL \n",
    "for url in df['URLs']:\n",
    "    # GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # parse the content with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # extract the publication time (hour)\n",
    "        date_span = soup.select_one('.contentSectionDetails span')\n",
    "        date_text = date_span.get_text(strip=True) if date_span else 'No time found'\n",
    "        \n",
    "        # extract the full article text\n",
    "        #from investing.com, it seems each paragraph is within a <p> tag directly under the class 'WYSIWYG articlePage'\n",
    "        article_paragraphs = soup.select('.WYSIWYG.articlePage p:not([class])')  # Selects <p> without any class attribute\n",
    "        full_article_text = ' '.join(p.get_text(strip=True) for p in article_paragraphs)\n",
    "        \n",
    "        # append what extracted to the lists\n",
    "        article_dates.append(date_text)\n",
    "        full_articles.append(full_article_text)\n",
    "    else:\n",
    "        # to handle unsuccessful requests\n",
    "        article_dates.append('Request failed')\n",
    "        full_articles.append('Request failed')\n",
    "\n",
    "df['Article Date'] = article_dates\n",
    "df['Full Article'] = full_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('full-article1.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('scraped_data2.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Sources', 'no1']] = df.Sources.str.split(\"-\",expand=True,)\n",
    "df[['no2', 'Sources']] = df.Sources.str.split(\"By \",expand=True,)\n",
    "df[['no3', 'Dates']] = df.Dates.str.split(\"-\",expand=True,)\n",
    "df= df[['Headlines','Sources','Dates','URLs']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store the scraped full articles\n",
    "article_dates = []\n",
    "full_articles = []\n",
    "\n",
    "\n",
    "# Loop over each URL \n",
    "for url in df['URLs']:\n",
    "    # GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # parse the content with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # extract the publication time (hour)\n",
    "        date_span = soup.select_one('.contentSectionDetails span')\n",
    "        date_text = date_span.get_text(strip=True) if date_span else 'No time found'\n",
    "        article_paragraphs = soup.select('.WYSIWYG.articlePage p:not([class])')  # Selects <p> without any class attribute\n",
    "        full_article_text = ' '.join(p.get_text(strip=True) for p in article_paragraphs)\n",
    "        \n",
    "        # append what extracted to the lists\n",
    "        article_dates.append(date_text)\n",
    "        full_articles.append(full_article_text)\n",
    "    else:\n",
    "        # to handle unsuccessful requests\n",
    "        article_dates.append('Request failed')\n",
    "        full_articles.append('Request failed')\n",
    "\n",
    "df['Article Date'] = article_dates\n",
    "df['Full Article'] = full_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('full-article2.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### R3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('scraped_data3.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Sources', 'no1']] = df.Sources.str.split(\"-\",expand=True,)\n",
    "df[['no2', 'Sources']] = df.Sources.str.split(\"By \",expand=True,)\n",
    "df[['no3', 'Dates']] = df.Dates.str.split(\"-\",expand=True,)\n",
    "df= df[['Headlines','Sources','Dates','URLs']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store the scraped full articles\n",
    "article_dates = []\n",
    "full_articles = []\n",
    "\n",
    "\n",
    "# Loop over each URL \n",
    "for url in df['URLs']:\n",
    "    # GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # parse the content with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # extract the publication time (hour)\n",
    "        date_span = soup.select_one('.contentSectionDetails span')\n",
    "        date_text = date_span.get_text(strip=True) if date_span else 'No time found'\n",
    "        article_paragraphs = soup.select('.WYSIWYG.articlePage p:not([class])')  # Selects <p> without any class attribute\n",
    "        full_article_text = ' '.join(p.get_text(strip=True) for p in article_paragraphs)\n",
    "        \n",
    "        # append what extracted to the lists\n",
    "        article_dates.append(date_text)\n",
    "        full_articles.append(full_article_text)\n",
    "    else:\n",
    "        # to handle unsuccessful requests\n",
    "        article_dates.append('Request failed')\n",
    "        full_articles.append('Request failed')\n",
    "\n",
    "df['Article Date'] = article_dates\n",
    "df['Full Article'] = full_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('full-article3.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### R4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('scraped_data4.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Sources', 'no1']] = df.Sources.str.split(\"-\",expand=True,)\n",
    "df[['no2', 'Sources']] = df.Sources.str.split(\"By \",expand=True,)\n",
    "df[['no3', 'Dates']] = df.Dates.str.split(\"-\",expand=True,)\n",
    "df= df[['Headlines','Sources','Dates','URLs']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store the scraped full articles\n",
    "article_dates = []\n",
    "full_articles = []\n",
    "\n",
    "\n",
    "# Loop over each URL \n",
    "for url in df['URLs']:\n",
    "    # GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # parse the content with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # extract the publication time (hour)\n",
    "        date_span = soup.select_one('.contentSectionDetails span')\n",
    "        date_text = date_span.get_text(strip=True) if date_span else 'No time found'\n",
    "        article_paragraphs = soup.select('.WYSIWYG.articlePage p:not([class])')  # Selects <p> without any class attribute\n",
    "        full_article_text = ' '.join(p.get_text(strip=True) for p in article_paragraphs)\n",
    "        \n",
    "        # append what extracted to the lists\n",
    "        article_dates.append(date_text)\n",
    "        full_articles.append(full_article_text)\n",
    "    else:\n",
    "        # to handle unsuccessful requests\n",
    "        article_dates.append('Request failed')\n",
    "        full_articles.append('Request failed')\n",
    "\n",
    "df['Article Date'] = article_dates\n",
    "df['Full Article'] = full_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('full-article4.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### R5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('scraped_data5.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Sources', 'no1']] = df.Sources.str.split(\"-\",expand=True,)\n",
    "df[['no2', 'Sources']] = df.Sources.str.split(\"By \",expand=True,)\n",
    "df[['no3', 'Dates']] = df.Dates.str.split(\"-\",expand=True,)\n",
    "df= df[['Headlines','Sources','Dates','URLs']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store the scraped full articles\n",
    "article_dates = []\n",
    "full_articles = []\n",
    "\n",
    "\n",
    "# Loop over each URL \n",
    "for url in df['URLs']:\n",
    "    # GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # parse the content with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # extract the publication time (hour)\n",
    "        date_span = soup.select_one('.contentSectionDetails span')\n",
    "        date_text = date_span.get_text(strip=True) if date_span else 'No time found'\n",
    "        article_paragraphs = soup.select('.WYSIWYG.articlePage p:not([class])')  # Selects <p> without any class attribute\n",
    "        full_article_text = ' '.join(p.get_text(strip=True) for p in article_paragraphs)\n",
    "        \n",
    "        # append what extracted to the lists\n",
    "        article_dates.append(date_text)\n",
    "        full_articles.append(full_article_text)\n",
    "    else:\n",
    "        # to handle unsuccessful requests\n",
    "        article_dates.append('Request failed')\n",
    "        full_articles.append('Request failed')\n",
    "\n",
    "df['Article Date'] = article_dates\n",
    "df['Full Article'] = full_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('full-article5.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### R6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('scraped_data6.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Sources', 'no1']] = df.Sources.str.split(\"-\",expand=True,)\n",
    "df[['no2', 'Sources']] = df.Sources.str.split(\"By \",expand=True,)\n",
    "df[['no3', 'Dates']] = df.Dates.str.split(\"-\",expand=True,)\n",
    "df= df[['Headlines','Sources','Dates','URLs']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store the scraped full articles\n",
    "article_dates = []\n",
    "full_articles = []\n",
    "\n",
    "\n",
    "# Loop over each URL \n",
    "for url in df['URLs']:\n",
    "    # GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # parse the content with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # extract the publication time (hour)\n",
    "        date_span = soup.select_one('.contentSectionDetails span')\n",
    "        date_text = date_span.get_text(strip=True) if date_span else 'No time found'\n",
    "        article_paragraphs = soup.select('.WYSIWYG.articlePage p:not([class])')  # Selects <p> without any class attribute\n",
    "        full_article_text = ' '.join(p.get_text(strip=True) for p in article_paragraphs)\n",
    "        \n",
    "        # append what extracted to the lists\n",
    "        article_dates.append(date_text)\n",
    "        full_articles.append(full_article_text)\n",
    "    else:\n",
    "        # to handle unsuccessful requests\n",
    "        article_dates.append('Request failed')\n",
    "        full_articles.append('Request failed')\n",
    "\n",
    "# add data to the df\n",
    "df['Article Date'] = article_dates\n",
    "df['Full Article'] = full_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('full-article6.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### R7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('scraped_data7.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Sources', 'no1']] = df.Sources.str.split(\"-\",expand=True,)\n",
    "df[['no2', 'Sources']] = df.Sources.str.split(\"By \",expand=True,)\n",
    "df[['no3', 'Dates']] = df.Dates.str.split(\"-\",expand=True,)\n",
    "df= df[['Headlines','Sources','Dates','URLs']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store the scraped full articles\n",
    "article_dates = []\n",
    "full_articles = []\n",
    "\n",
    "\n",
    "# Loop over each URL \n",
    "for url in df['URLs']:\n",
    "    # GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # parse the content with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # extract the publication time (hour)\n",
    "        date_span = soup.select_one('.contentSectionDetails span')\n",
    "        date_text = date_span.get_text(strip=True) if date_span else 'No time found'\n",
    "        article_paragraphs = soup.select('.WYSIWYG.articlePage p:not([class])')  # Selects <p> without any class attribute\n",
    "        full_article_text = ' '.join(p.get_text(strip=True) for p in article_paragraphs)\n",
    "        \n",
    "        # append what extracted to the lists\n",
    "        article_dates.append(date_text)\n",
    "        full_articles.append(full_article_text)\n",
    "    else:\n",
    "        # to handle unsuccessful requests\n",
    "        article_dates.append('Request failed')\n",
    "        full_articles.append('Request failed')\n",
    "\n",
    "# add data to the df\n",
    "df['Article Date'] = article_dates\n",
    "df['Full Article'] = full_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('full-article7.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### R8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('scraped_data8.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Sources', 'no1']] = df.Sources.str.split(\"-\",expand=True,)\n",
    "df[['no2', 'Sources']] = df.Sources.str.split(\"By \",expand=True,)\n",
    "df[['no3', 'Dates']] = df.Dates.str.split(\"-\",expand=True,)\n",
    "df= df[['Headlines','Sources','Dates','URLs']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store the scraped full articles\n",
    "article_dates = []\n",
    "full_articles = []\n",
    "\n",
    "\n",
    "# Loop over each URL \n",
    "for url in df['URLs']:\n",
    "    # GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # parse the content with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # extract the publication time (hour)\n",
    "        date_span = soup.select_one('.contentSectionDetails span')\n",
    "        date_text = date_span.get_text(strip=True) if date_span else 'No time found'\n",
    "        article_paragraphs = soup.select('.WYSIWYG.articlePage p:not([class])')  # Selects <p> without any class attribute\n",
    "        full_article_text = ' '.join(p.get_text(strip=True) for p in article_paragraphs)\n",
    "        \n",
    "        # append what extracted to the lists\n",
    "        article_dates.append(date_text)\n",
    "        full_articles.append(full_article_text)\n",
    "    else:\n",
    "        # to handle unsuccessful requests\n",
    "        article_dates.append('Request failed')\n",
    "        full_articles.append('Request failed')\n",
    "\n",
    "# add data to the df\n",
    "df['Article Date'] = article_dates\n",
    "df['Full Article'] = full_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('full-article8.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### R9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('scraped_data9.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Sources', 'no1']] = df.Sources.str.split(\"-\",expand=True,)\n",
    "df[['no2', 'Sources']] = df.Sources.str.split(\"By \",expand=True,)\n",
    "df[['no3', 'Dates']] = df.Dates.str.split(\"-\",expand=True,)\n",
    "df= df[['Headlines','Sources','Dates','URLs']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store the scraped full articles\n",
    "article_dates = []\n",
    "full_articles = []\n",
    "\n",
    "\n",
    "# Loop over each URL \n",
    "for url in df['URLs']:\n",
    "    # GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # parse the content with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # extract the publication time (hour)\n",
    "        date_span = soup.select_one('.contentSectionDetails span')\n",
    "        date_text = date_span.get_text(strip=True) if date_span else 'No time found'\n",
    "        article_paragraphs = soup.select('.WYSIWYG.articlePage p:not([class])')  # Selects <p> without any class attribute\n",
    "        full_article_text = ' '.join(p.get_text(strip=True) for p in article_paragraphs)\n",
    "        \n",
    "        # append what extracted to the lists\n",
    "        article_dates.append(date_text)\n",
    "        full_articles.append(full_article_text)\n",
    "    else:\n",
    "        # to handle unsuccessful requests\n",
    "        article_dates.append('Request failed')\n",
    "        full_articles.append('Request failed')\n",
    "\n",
    "# add data to the df\n",
    "df['Article Date'] = article_dates\n",
    "df['Full Article'] = full_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('full-article9.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### R10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('scraped_data10.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Sources', 'no1']] = df.Sources.str.split(\"-\",expand=True,)\n",
    "df[['no2', 'Sources']] = df.Sources.str.split(\"By \",expand=True,)\n",
    "df[['no3', 'Dates']] = df.Dates.str.split(\"-\",expand=True,)\n",
    "df= df[['Headlines','Sources','Dates','URLs']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store the scraped full articles\n",
    "article_dates = []\n",
    "full_articles = []\n",
    "\n",
    "\n",
    "# Loop over each URL \n",
    "for url in df['URLs']:\n",
    "    # GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # parse the content with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # extract the publication time (hour)\n",
    "        date_span = soup.select_one('.contentSectionDetails span')\n",
    "        date_text = date_span.get_text(strip=True) if date_span else 'No time found'\n",
    "        article_paragraphs = soup.select('.WYSIWYG.articlePage p:not([class])')  # Selects <p> without any class attribute\n",
    "        full_article_text = ' '.join(p.get_text(strip=True) for p in article_paragraphs)\n",
    "        \n",
    "        # append what extracted to the lists\n",
    "        article_dates.append(date_text)\n",
    "        full_articles.append(full_article_text)\n",
    "    else:\n",
    "        # to handle unsuccessful requests\n",
    "        article_dates.append('Request failed')\n",
    "        full_articles.append('Request failed')\n",
    "\n",
    "# add data to the df\n",
    "df['Article Date'] = article_dates\n",
    "df['Full Article'] = full_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('full-article10.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### R11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('scraped_data11.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Sources', 'no1']] = df.Sources.str.split(\"-\",expand=True,)\n",
    "df[['no2', 'Sources']] = df.Sources.str.split(\"By \",expand=True,)\n",
    "df[['no3', 'Dates']] = df.Dates.str.split(\"-\",expand=True,)\n",
    "df= df[['Headlines','Sources','Dates','URLs']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store the scraped full articles\n",
    "article_dates = []\n",
    "full_articles = []\n",
    "\n",
    "\n",
    "# Loop over each URL \n",
    "for url in df['URLs']:\n",
    "    # GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # parse the content with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # extract the publication time (hour)\n",
    "        date_span = soup.select_one('.contentSectionDetails span')\n",
    "        date_text = date_span.get_text(strip=True) if date_span else 'No time found'\n",
    "        article_paragraphs = soup.select('.WYSIWYG.articlePage p:not([class])')  # Selects <p> without any class attribute\n",
    "        full_article_text = ' '.join(p.get_text(strip=True) for p in article_paragraphs)\n",
    "        \n",
    "        # append what extracted to the lists\n",
    "        article_dates.append(date_text)\n",
    "        full_articles.append(full_article_text)\n",
    "    else:\n",
    "        # to handle unsuccessful requests\n",
    "        article_dates.append('Request failed')\n",
    "        full_articles.append('Request failed')\n",
    "\n",
    "# add data to the df\n",
    "df['Article Date'] = article_dates\n",
    "df['Full Article'] = full_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('full-article11.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### R12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('scraped_data12.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Sources', 'no1']] = df.Sources.str.split(\"-\",expand=True,)\n",
    "df[['no2', 'Sources']] = df.Sources.str.split(\"By \",expand=True,)\n",
    "df[['no3', 'Dates']] = df.Dates.str.split(\"-\",expand=True,)\n",
    "df= df[['Headlines','Sources','Dates','URLs']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store the scraped full articles\n",
    "article_dates = []\n",
    "full_articles = []\n",
    "\n",
    "\n",
    "# Loop over each URL \n",
    "for url in df['URLs']:\n",
    "    # GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # parse the content with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # extract the publication time (hour)\n",
    "        date_span = soup.select_one('.contentSectionDetails span')\n",
    "        date_text = date_span.get_text(strip=True) if date_span else 'No time found'\n",
    "        article_paragraphs = soup.select('.WYSIWYG.articlePage p:not([class])')  # Selects <p> without any class attribute\n",
    "        full_article_text = ' '.join(p.get_text(strip=True) for p in article_paragraphs)\n",
    "        \n",
    "        # append what extracted to the lists\n",
    "        article_dates.append(date_text)\n",
    "        full_articles.append(full_article_text)\n",
    "    else:\n",
    "        # to handle unsuccessful requests\n",
    "        article_dates.append('Request failed')\n",
    "        full_articles.append('Request failed')\n",
    "\n",
    "# add data to the df\n",
    "df['Article Date'] = article_dates\n",
    "df['Full Article'] = full_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('full-article12.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### R13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('scraped_data13.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Sources', 'no1']] = df.Sources.str.split(\"-\",expand=True,)\n",
    "df[['no2', 'Sources']] = df.Sources.str.split(\"By \",expand=True,)\n",
    "df[['no3', 'Dates']] = df.Dates.str.split(\"-\",expand=True,)\n",
    "df= df[['Headlines','Sources','Dates','URLs']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store the scraped full articles\n",
    "article_dates = []\n",
    "full_articles = []\n",
    "\n",
    "\n",
    "# Loop over each URL \n",
    "for url in df['URLs']:\n",
    "    # GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # parse the content with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # extract the publication time (hour)\n",
    "        date_span = soup.select_one('.contentSectionDetails span')\n",
    "        date_text = date_span.get_text(strip=True) if date_span else 'No time found'\n",
    "        article_paragraphs = soup.select('.WYSIWYG.articlePage p:not([class])')  # Selects <p> without any class attribute\n",
    "        full_article_text = ' '.join(p.get_text(strip=True) for p in article_paragraphs)\n",
    "        \n",
    "        # append what extracted to the lists\n",
    "        article_dates.append(date_text)\n",
    "        full_articles.append(full_article_text)\n",
    "    else:\n",
    "        # to handle unsuccessful requests\n",
    "        article_dates.append('Request failed')\n",
    "        full_articles.append('Request failed')\n",
    "\n",
    "df['Article Date'] = article_dates\n",
    "df['Full Article'] = full_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('full-article13.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### R14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('scraped_data14.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Sources', 'no1']] = df.Sources.str.split(\"-\",expand=True,)\n",
    "df[['no2', 'Sources']] = df.Sources.str.split(\"By \",expand=True,)\n",
    "df[['no3', 'Dates']] = df.Dates.str.split(\"-\",expand=True,)\n",
    "df= df[['Headlines','Sources','Dates','URLs']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store the scraped full articles\n",
    "article_dates = []\n",
    "full_articles = []\n",
    "\n",
    "\n",
    "# Loop over each URL \n",
    "for url in df['URLs']:\n",
    "    # GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # parse the content with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # extract the publication time (hour)\n",
    "        date_span = soup.select_one('.contentSectionDetails span')\n",
    "        date_text = date_span.get_text(strip=True) if date_span else 'No time found'\n",
    "        article_paragraphs = soup.select('.WYSIWYG.articlePage p:not([class])')  # Selects <p> without any class attribute\n",
    "        full_article_text = ' '.join(p.get_text(strip=True) for p in article_paragraphs)\n",
    "        \n",
    "        # append what extracted to the lists\n",
    "        article_dates.append(date_text)\n",
    "        full_articles.append(full_article_text)\n",
    "    else:\n",
    "        # to handle unsuccessful requests\n",
    "        article_dates.append('Request failed')\n",
    "        full_articles.append('Request failed')\n",
    "\n",
    "df['Article Date'] = article_dates\n",
    "df['Full Article'] = full_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('full-article14.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### R15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('scraped_data15.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Sources', 'no1']] = df.Sources.str.split(\"-\",expand=True,)\n",
    "df[['no2', 'Sources']] = df.Sources.str.split(\"By \",expand=True,)\n",
    "df[['no3', 'Dates']] = df.Dates.str.split(\"-\",expand=True,)\n",
    "df= df[['Headlines','Sources','Dates','URLs']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store the scraped full articles\n",
    "article_dates = []\n",
    "full_articles = []\n",
    "\n",
    "\n",
    "# Loop over each URL \n",
    "for url in df['URLs']:\n",
    "    # GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # parse the content with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # extract the publication time (hour)\n",
    "        date_span = soup.select_one('.contentSectionDetails span')\n",
    "        date_text = date_span.get_text(strip=True) if date_span else 'No time found'\n",
    "        article_paragraphs = soup.select('.WYSIWYG.articlePage p:not([class])')  # Selects <p> without any class attribute\n",
    "        full_article_text = ' '.join(p.get_text(strip=True) for p in article_paragraphs)\n",
    "        \n",
    "        # append what extracted to the lists\n",
    "        article_dates.append(date_text)\n",
    "        full_articles.append(full_article_text)\n",
    "    else:\n",
    "        # to handle unsuccessful requests\n",
    "        article_dates.append('Request failed')\n",
    "        full_articles.append('Request failed')\n",
    "\n",
    "# add data to the df\n",
    "df['Article Date'] = article_dates\n",
    "df['Full Article'] = full_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('full-article15.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### R16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('scraped_data16.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Sources', 'no1']] = df.Sources.str.split(\"-\",expand=True,)\n",
    "df[['no2', 'Sources']] = df.Sources.str.split(\"By \",expand=True,)\n",
    "df[['no3', 'Dates']] = df.Dates.str.split(\"-\",expand=True,)\n",
    "df= df[['Headlines','Sources','Dates','URLs']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store the scraped full articles\n",
    "article_dates = []\n",
    "full_articles = []\n",
    "\n",
    "\n",
    "# Loop over each URL \n",
    "for url in df['URLs']:\n",
    "    # GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # parse the content with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # extract the publication time (hour)\n",
    "        date_span = soup.select_one('.contentSectionDetails span')\n",
    "        date_text = date_span.get_text(strip=True) if date_span else 'No time found'\n",
    "        article_paragraphs = soup.select('.WYSIWYG.articlePage p:not([class])')  # Selects <p> without any class attribute\n",
    "        full_article_text = ' '.join(p.get_text(strip=True) for p in article_paragraphs)\n",
    "        \n",
    "        # append what extracted to the lists\n",
    "        article_dates.append(date_text)\n",
    "        full_articles.append(full_article_text)\n",
    "    else:\n",
    "        # to handle unsuccessful requests\n",
    "        article_dates.append('Request failed')\n",
    "        full_articles.append('Request failed')\n",
    "\n",
    "# add data to the df\n",
    "df['Article Date'] = article_dates\n",
    "df['Full Article'] = full_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('full-article16.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### R17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('scraped_data17.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Sources', 'no1']] = df.Sources.str.split(\"-\",expand=True,)\n",
    "df[['no2', 'Sources']] = df.Sources.str.split(\"By \",expand=True,)\n",
    "df[['no3', 'Dates']] = df.Dates.str.split(\"-\",expand=True,)\n",
    "df= df[['Headlines','Sources','Dates','URLs']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store the scraped full articles\n",
    "article_dates = []\n",
    "full_articles = []\n",
    "\n",
    "\n",
    "# Loop over each URL \n",
    "for url in df['URLs']:\n",
    "    # GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # parse the content with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # extract the publication time (hour)\n",
    "        date_span = soup.select_one('.contentSectionDetails span')\n",
    "        date_text = date_span.get_text(strip=True) if date_span else 'No time found'\n",
    "        article_paragraphs = soup.select('.WYSIWYG.articlePage p:not([class])')  # Selects <p> without any class attribute\n",
    "        full_article_text = ' '.join(p.get_text(strip=True) for p in article_paragraphs)\n",
    "        \n",
    "        # append what extracted to the lists\n",
    "        article_dates.append(date_text)\n",
    "        full_articles.append(full_article_text)\n",
    "    else:\n",
    "        # to handle unsuccessful requests\n",
    "        article_dates.append('Request failed')\n",
    "        full_articles.append('Request failed')\n",
    "\n",
    "# add data to the df\n",
    "df['Article Date'] = article_dates\n",
    "df['Full Article'] = full_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('full-article17.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### concatnation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1= pd.read_excel('full-article1.xlsx')\n",
    "df2= pd.read_excel('full-article2.xlsx')\n",
    "df3= pd.read_excel('full-article3.xlsx')\n",
    "df4= pd.read_excel('full-article4.xlsx')\n",
    "df5= pd.read_excel('full-article5.xlsx')\n",
    "df6= pd.read_excel('full-article6.xlsx')\n",
    "df7= pd.read_excel('full-article7.xlsx')\n",
    "df8= pd.read_excel('full-article8.xlsx')\n",
    "df9= pd.read_excel('full-article9.xlsx')\n",
    "df10= pd.read_excel('full-article10.xlsx')\n",
    "df11= pd.read_excel('full-article11.xlsx')\n",
    "df12= pd.read_excel('full-article12.xlsx')\n",
    "df13= pd.read_excel('full-article13.xlsx')\n",
    "df14= pd.read_excel('full-article14.xlsx')\n",
    "df15= pd.read_excel('full-article15.xlsx')\n",
    "df16= pd.read_excel('full-article16.xlsx')\n",
    "df17= pd.read_excel('full-article17.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.concat([df1, df2], ignore_index= True)\n",
    "x = pd.concat([x, df3], ignore_index= True)\n",
    "x = pd.concat([x, df4], ignore_index= True)\n",
    "x = pd.concat([x, df5], ignore_index= True)\n",
    "x = pd.concat([x, df6], ignore_index= True)\n",
    "x = pd.concat([x, df7], ignore_index= True)\n",
    "x = pd.concat([x, df8], ignore_index= True)\n",
    "x = pd.concat([x, df9], ignore_index= True)\n",
    "x = pd.concat([x, df10], ignore_index= True)\n",
    "x = pd.concat([x, df11], ignore_index= True)\n",
    "x = pd.concat([x, df12], ignore_index= True)\n",
    "x = pd.concat([x, df13], ignore_index= True)\n",
    "x = pd.concat([x, df14], ignore_index= True)\n",
    "x = pd.concat([x, df15], ignore_index= True)\n",
    "x = pd.concat([x, df16], ignore_index= True)\n",
    "x = pd.concat([x, df17], ignore_index= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= x[1:]\n",
    "x['Dates'] = pd.to_datetime(x['Dates'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove duplicates: keep the first occurrence within the same date\n",
    "x1= x.drop_duplicates(subset=['Dates', 'Headlines'], keep='first')\n",
    "len(x), len(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = x1.drop(columns=['Unnamed: 0'])\n",
    "x1 = x1.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1.to_excel('news_sample.xlsx')\n",
    "x1.to_csv('news_sample.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
